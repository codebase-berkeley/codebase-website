aptiv:
  intro: >
    Aptiv is a global technology company that develops safer, greener, and more connected solutions, which enable the future of mobility. Aptiv is delivering the software capabilities, advanced computing platforms, and networking architecture to assist smart and self driving cars.
  problem: >
    Develop a driver takeover readiness heuristic using various ML models and showcase this heuristic with real data in a Unity simulation.
  implementation: >
    First there was the Unity simulation, which we built from scratch. We worked on the simulation throughout the semester, first setting up basic functionality to display important information from the dataset and then polishing up the UI. Second, we worked on the driver takeover readiness heuristic, which composed of decided what would be valuable to the score and then implementing the necessary models for those values in Python.
  challenges: >
    As with any dataset, there's a lot of cleaning to do before using it in a model or simulation. Aptiv's dataset was cumbersome to work with because it wasn't a simple MxN dimensional dataset, since the dataset was a time series and each sample was itself a variable dimension matrix, which made writing code for cleaning the dataset much more cumbersome. Aptiv's tracking system also had many nuances, further making cleaning and working with their dataset more difficult. The Unity simulation also had its own challenges, such as interpolating surrounding vehicle objects and removing jitteriness from their movements.
  takeaways: >
    This project's scope ended up changing multiple times over the course of the semester, which made our objectives unclear in the beginning. More detailed objectives on our part and clearer expectations on Aptiv's part would have significantly hastened our project development. Also, having more Aptiv engineers available to clarify technical specifications would have been very useful throughout our project.
  extra: ""
  image: none

atlassian:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

beam:
  intro: >
    Beam Financial is a mobile-first online banking platform that offers 2-4% APY on your assets stored in their system. Through the Beam app, users can invite their friends to gain daily interest rate rewards, called Billies. The platform is zero-fees, no interest rate, and FDIC insured.
  problem: >
    During our working period with Beam, their lean engineering team was fully focused on the initial launch of their application to the iOS App Store. Many of the backend and database systems were in place, but the engineers had no easy way to query for and visualize their data.
    <br> <br> Our job was to build, from scratch, an internal facing administration dashboard for various Beam data. This included a standardized table view of Customers, Transactions, Interest Rates, etc. as well as visualizations and core metrics for Beam to track their performance over time. We were to own this project from end-to-end - everything from initial Google OAuth authentication to deploying the application on their Heroku instances.
  implementation: >
    The implementation of the project happened in 2 pieces. The first of which was a frontend in React.js and TypeScript that queried data from our backend to display in various formats and communicated with AWS Cognito and Google OAuth for user authentication. The second was our GraphQL backend layer, built on top of Beam’s own Postgres databases. We implemented a one-to-one GraphQL mapping with Postgres, implemented our own custom endpoints, and also ran daily CRON jobs for aggregation metrics.
  challenges: >
    Once we moved our development process in to Beam’s QA database, we became painfully aware that many of our data operations could not be handled on the client side. This meant ripping out much of our frontend code, and implementing, among other things, server-side data pagination, server-side data sorting, and server-side fuzzy search. These technical challenges, combined with our relative unfamiliarity with GraphQL, proved to be a time-consuming roadblock.
    <br> <br> The authentication flow in our application resulted in quite a bit of churn in our application. We were trying to integrate AWS Cognito, Google OAuth 2.0, and both our React application and backend GraphQL servers all into one seamless workflow. Our users were to hit a Google sign in link on our React application, have it be okayed by Google OAuth, redirect to AWS Cognito, quickly confirm with our GraphQL layer, swing back to AWS Cognito, and then finally confirm with our React frontend. There was no simple fix for this and we had to get our hands dirty with each request in the pipeline.
  takeaways: >
    In retrospect, there were still a couple pieces of the project specification that were not clear, and we ran into some trouble near the end of the project trying to clarify expectations. Beam did not provide a dedicated designer on the application and we should have allocated more design resources, on our end, towards the project.
    <br> <br> Everyone on the team still learned a lot from the experience. It’s not often a team gets to own the entire life of an application from end-to-end like we did. Building a web application with concretely definable tasks allowed us to focus on operations, coding practices, and teamwork. Our final application was running on real Beam data and offered real, tangible insights for Beam employees.
  extra:
  image: none

bettercloud:
  intro: >
    BetterCloud is a SaaS management platform headquartered in New York. IT admins can automate processes to enforce company policies, onboard employees, and complete other IT duties. Many companies, such as BuzzFeed and Zuora, use BetterCloud’s product.
  problem: >
    We tackled two independent problems for BetterCloud.
    <br><br>Many IT admins use ServiceNow for managing and tracking tickets (issues to be solved by the IT team). IT admins who also use BetterCloud’s services had to switch between the ServiceNow and BetterCloud platforms many times throughout the process. So, we integrated BetterCloud’s functionalities within the ServiceNow platform by creating a custom ServiceNow developer instance. Users were able to link and start BetterCloud workflows from within a ServiceNow ticket.
    <br><br>Our second problem involved proving a distributed server concept. BetterCloud works with many different vendors’ APIs. For example, in order to complete a workflow, BetterCloud might have to use Google, Slack and Okta APIs in order to have complete authentication. However, these vendors might impose a limit to how many API calls a server can make over a certain time period, also know as a rate limit. As BetterCloud grows, rate limiting is becoming an issue, since an increasing number of API calls is needed. This is in danger of leading to BetterCloud service failures, since rate limits can be easily exceeded. Thus, we worked on ways to combat this issue and came up with two approaches that can be combined in the future. In our first approach, we distributed the load across several servers with a specified rate created using Google Cloud Functions. In our second approach, we had a queue to keep track of the number of remaining requests to be made and requeued failed requests.
  implementation: >
    We worked within ServiceNow’s platform for the first project, integrating BetterCloud workflows into ServiceNow, using JavaScript. For our second project, we used Node.js and Google Cloud Functions to create a customizable to for API rate limit testing.
  challenges: >
    ServiceNow’s technologies were very niche and had a steep learning curve. Learning how to work with their interface was definitely a challenge, since oftentimes it was very buggy. ServiceNow has its own version control system, which turned out to be difficult to work with. There was no simple way to merge work from different sub-projects on GitHub, which was a blocker for the first half of the semester.
    <br><br> For the second project, replicating actual API rate limit calling was relatively difficult, because we weren’t provided with production-level BetterCloud endpoints, limiting our ability to build a precise tool that could be exactly used for BetterCloud’s purposes.
  takeaways: >
    In retrospect, we could’ve committed ourselves to more work. Overall, everyone learned a lot from this experience and it’s awesome to see how essential our work was to BetterCloud’s growth.
  extra: ""
  image: none

hackerrank:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

mentored-sp18:
  intro: In Spring 2018, the mentored project was Codedoor, a clone of Glassdoor for internal use. Codebase members can log into the web app using their slack accounts and, like in Glassdoor, post and view reviews, ratings, and interviews for different companies.
  problem: Career development is one reason why many people join Codebase, and in particular what many members want is a resource for what companies are worth applying to, what it’s like to work for them, and how hard their interviews are. Glassdoor is a great public resource for this, and we agreed that having an internal version of Glassdoor would be a great investment for the club.
           <br> <br> The primary task of the mentored project members was to build a web application from scratch including all the necessary profiles, companies, and pages. The design was completed by project managers before the project began and the problem boiled down to translating designs into working pages, with a home page displaying popular companies, a page for each company listing their reviews and interviews, a search function for interviews, companies, and reviews, and finally, a permalink page for each interview and review.
  implementation: Codedoor was implemented on top of Django, a tried-and-true Python web framework used by companies like Instagram and Pinterest. Some basic interactivity was added using vanilla Javascript to the dynamic web pages served by the backend. With only about ten weeks to both teach fundamentals of web development and build the app itself, we chose to not use a Javascript framework like React or Vue.js because of the learning curve imposed by such frameworks and the fact that many mentored members had only just been exposed to basic HTML and CSS.
  challenges: Our goal had been to curate the technical content of this project closely to provide the most learning possible. This meant that the mentored project managers had to create a prototype of the application before the members built out the production version, so that crucial design choices like schemas, flows, and frameworks used could be decided upon in advance; as a result, we were able to carry out the project with few unexpected hiccups and could focus on building out our product in the best way possible, with both project managers holding comprehensive code reviews on feature pull requests.
              <br> <br> One road bump that we did hit was when we began to obtain company data for Codedoor. To populate our database with companies, we needed to build a script that scraped Crunchbase’s public top-ranked companies. We did have access to a CSV file with the top 50,000 companies from about five years ago, but the data was incomplete and many features like logo images were missing. Using the web scraping script and the help of a public logo API, we were able to impute the data we needed, but we did have to redesign some pages and forms - replacing things like company dropdowns with search boxes - to offer a smooth user experience in terms of selecting and searching for tens of thousands of companies.
  takeaways: Ultimately, the mentored project is less about the product built and more about the things learned. Each and every mentored member exceeded expectations in translating a design into real code, and, by building out everything from full-text search on the backend to the homepage feed on the frontend, was able to gain substantial experience as an engineer working across the web stack.
             <br> <br> In retrospect, using a modern Javascript framework like React as a base would have been a better idea than going with Django again. For the next mentored project, we’ll be using React, and accordingly the decoupled backend will be able to be written in any language or framework and will support a mobile app far more easily.
  extra: ""
  image: none

mentored-fa18:
  intro: >
    The Mentored Project is a unique project in which students with little to no computer science background learn full-stack web development through working together to engineer a web application for an external client. In Fall 2018, the Mentored Project built a web application for the teachers of Ygnacio Valley High School that would help them easily manage and track their lesson plans. Click <a href="https://medium.com/@codebase/the-mentored-project-72db8aabb70e?fbclid=IwAR0qIYNmioBSdjvqgu-yvzsgkIg3FYkJU9KNt-pWzIEnLxzaO6FLOoNBZZ4">here</a> to read more about the project and the curriculum.
  problem: >
    During our working period with the high school, we were to create a web application in which teachers could create classes, units, and upload lessons. Teachers would also be able to create multiple choice quizzes in order to quiz students on the lesson. Students would be able to log into the application and take the quizzes. Teachers would also have a place to visualize the quiz data over time.
  implementation: >
    In a full-stack web application, there is a frontend and a backend component. For the frontend we used React.js and divided the web application into multiple components. We passed data  between components using props and state variables. For the backend, we used Express.js for routing and managed the data using a PostgreSQL database. Through learning these various technologies, members were able to gain a solid understanding of full-stack web development.
  challenges: >
    One of the hardest things to implement was login via Google OAuth. Creating and storing users was no easy task, but developers Isabelle Zhou and Kelvin Jue persisted and figured it out. <a href="https://docs.google.com/document/d/1MYzSBgsyY3Gie2E7DI6NqYeveSXEhXryZ9lvf-x3Hlg/edit">Here</a> is some documentation they created on how to implement login with a React, Express, PostgreSQL stack.
    <br><br>We also struggled with passing a lot of data between React components. We had to properly organize our variables and pass down functions to child components that would call the parent component to properly update the variable.

  takeaways: >
    Mentored Project is special because of the sheer amount that developers learn from the experience. After a semester on Mentored Project and learning web development, they are able to tackle future client projects in CodeBase. It was great to see developers show persistence through difficult technical obstacles in order to find a solution. That tenacity and love for technical challenges are what make a great engineer.
  extra: ""
  image: none

netskope:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

pantheon:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

propelx:
  intro: >
    Propel(x) is an investing platform that allows investors to browse and fund promising startups. Using the platform, investors can view startup pitch decks, ask questions about the startups, view detailed information about the startups, and commit to investments.
  problem: >
    Propel(x) originally had a desktop-only interface and were interested in exploring different ways of communicating information to their users. They specifically wanted to target users who were browsing on-the-go on their phones, and to get them interacting with the platform even when they weren’t near a laptop.
    <br> <br> Our job was to build a mobile application that gives users the ability to access certain core features of the desktop interface. This includes a Tinder-like home view where investors can swipe through startup cards. Pressing on startup cards they are interested in brings users to an expanded view that allows them to access detailed startup information. In addition, users are able to save startups for later and view them in a list.
  implementation: >
    The mobile app was written in React Native, and we used a wide variety of React Native packages to get the UI components needed for the app. To hook our frontend up to Propel(x)’s backend, we were given several endpoints to work with, which gave developers the opportunity to work with REST and figure out how to parse endpoint responses.
  challenges: >
    One of our biggest challenges had to do with managing state information. We decided not to use Redux to manage state for a few reasons. For one, the app is relatively lightweight in terms of the information we have to pull from the backend. Additionally, every single developer on the team was new to React Native, and with only 10 weeks or so for the project, I decided to have them focus on learning the core technology and not have to worry about Redux (which has a bit of a learning curve). However, this meant that we did not have a central store of information and a single source of truth for everything happening in the app, and would have to have individual pages communicate changes in state to each other in the frontend. We created a workaround by setting our source of truth to be a frontend component, which acted as a central store, making GET and POST requests to the backend, and sending and receiving information to and from the individual pages in the app.
    <br> <br> Another challenge was figuring out how to implement swiping. Most existing React Native packages for swiping were a little finicky and didn’t fit our use case well. We had to apply some creative styling and fixes to existing packages to get swiping to function the way the client wanted it to.
  takeaways: >
    The Propel(x) project was a wonderful learning experience for everybody involved—the client enjoyed the opportunity to build relationships with college students, the developers all picked up new skills and grew as engineers as they tackled challenge after challenge, and I gained a lot of insight into what it means to lead and manage a team. In retrospect, there were a few things that could have been done better (maybe it would have been worth it from an engineering and code style standpoint to invest time into learning Redux, or maybe we could have worked with Propel(x) to develop a UI equally as interesting as swiping but not as finicky and difficult to wrangle together), but overall the partnership was successful and we were able to deliver on all the functionality that we promised! At the conclusion of the project, Propel(x) hired a mobile engineer to take over the app development, and hopes to release a production version soon.
  extra:
  image: none

polymorph-ua:
  intro: >
    Polymorph is an ad-tech startup based in SF, and it focuses on creating a white-label suite for publishers who want to create, manage, and display ads on their own websites and applications. Polymorph focuses on native ads, which are ads that fit within the context of the page. So clients who use Polymorph want to use certain areas of their page to display ads to earn revenue, and Polymorph facilitates this process of choosing which ads to display to maximize revenue. Polymorph’s platform is white-labelled, meaning that the ads appear as if they were created by the site displaying the ads, meaning these ad insertions feel natural for users. The company’s suite currently serves 15,000 + sites/apps, and can process 100,00 ad requests per second.
  problem: >
    In order to serve both advertisers and publishers who want to sell ad space, Polymorph holds auctions for advertisers to bid on ad spaces to place their ads. Polymorph conducts both direct auctions and third-party auctions. For direct auctions, Polymorph handles the bidding for the advertisers, and uses stats and metrics from these advertisers to calculate a bidding price. For third-party auctions, Polymorph receives bids from third-party advertisers and uses the bids set by these third-parties.
    <br> <br> There are multiple units used for bidding, two of which are CPC (cost per click) and CPM (cost per thousand ads loaded). Direct auctions allow bids in both CPC and CPM, while third-party only allow bids in CPM.  Our job was to predict CTR, which is the probability a user clicks on an ad given that the ad is loaded on their webpage. The purpose of this project is to efficiently and accurately convert between the above-mentioned two units of measurement for bidding since direct auctions has bids using both units.
  implementation: >
    This being an ML project, we first looked at existing papers on calculating CPC. Luckily, there have already been previous competitions and attempts to calculate CPC and we tried creating models based on previous literature. Before getting started, we analyzed the data given to us with exploratory data analysis to visually determine useful features and randomly subsampled features hundreds of times for feature selection. We experimented with 4 models: field-aware factorization machines, random forests, logistic regression, and gradient boosting. In order to measure accuracy, we used various units of measurements, such as log-loss and f1 score, and also used grid search for hyperparameter tuning.
  challenges: >
    The biggest technical challenge was both the lack of data and the poor quality of the data. The initial data set given had too few positive samples (data for clicked ads), so models always returned a prediction of no-click since it didn’t have enough data for clicked ads. Additionally, there weren’t too many useful features in that many of the features given (such as the country and browser of the user) did not tell us much in terms of whether or not the user would click the ad. Thus, our models simply didn’t have enough quality data to train on and this threw them off considerably. After the mid-semester deliverable, we were able to get more and more specific data, and this helped our models.
    <br> <br>  Another technical challenge was standardizing feature selection and hyperparameter values. Everyone took ownership of a separate model and worked in a separate ipython notebook, so near the end, people had different procedures for selecting features and tuning hyperparameters. Making sure that we standardized these procedures was essential in ensuring final results were comparable.
  takeaways: >
    In retrospect, the project was intriguing conceptually, and we had a lot of fun trying to predict CTR, but there were still many roadblocks we faced given the data issues and the fact that with such a large dataset, iterative changes took days.
    <br> <br> Everyone on the team still learned a lot from the experience. Many members did not have much previous exposure to machine learning, and this was a very practical and face-paced, albeit rocky, introduction to machine learning. Learning about the pipeline for creating a working machine learning model was incredibly intellectually stimulating, in that the actual model is only the tip of the iceberg-to create such a model. Hours and hours of data cleaning and examination, parameter tuning, and model training were required. Often times, doing this randomly until the best possible result emerge actually worked the best.
  extra:
  image: none

polymorph-dpf:
  intro: >
    Polymorph is an ad-tech startup based in SF that maintains a white-label suite for publishers who want to create, manage, and display ads on their own websites and applications. Polymorph focuses on native ads, which are ads that fit within the context of the page. Clients who use Polymorph want to use certain areas of their page to display ads to earn revenue, and Polymorph facilitates this process of choosing which ads to display to maximize revenue. Polymorph’s platform is white-labelled, meaning that the ads appear as if they were created by the site displaying the ads and that these ad insertions feel natural for users. The company’s suite currently serves more than 15,000 sites/apps, and can process billions of ad requests a day.
  problem: >
    Polymorph acts as a <a href="https://en.wikipedia.org/wiki/Supply-side_platform">supply-side platform</a> that conducts real-time, second-price auctions on behalf of clients. In a second-price auction, the winner pays the second-highest bid.
    <br> <br> Polymorph provides publishers the opportunity to set a price floor to protect valuation of inventory and generate additional revenue. In the case of a single RTB auction, a price floor lifts revenue if it falls between the top two bids, and lowers revenue if it falls above the highest bid. For some perspective, a bid might fall in the range $0.0000001 to $0.10.
    <br> <br> Under the reasonable assumption that a publisher's optimal (highest-revenue) static price floor does not vary significantly day-to-day, revenue may be optimized by manually tweaking the floor on a per-placement basis. However, dynamically adjusting price floor based on past bid amounts and known information about the auction has the potential to not only reduce maintenance time but also to lift publisher revenue.
  implementation: >
    Given ad auction data with static reserve prices in effect, we researched, implemented, and evaluated several algorithms for setting dynamic price floors to lift publisher revenue. Some pricing strategies were based off of RTB research papers, while others used ML frameworks such as VW and Tensorflow to inform floors. We created a simulator that ran strategies on our test data in order to calculate statistics for each strategy, and standardize our results.
  challenges: >
    We received data recorded with static price floors in place, meaning that bids that would have been below the price floor were not submitted. Because price floors were often engaged, most auctions did not have more than one recorded bid. To accurately simulate the effect of a pricing strategy however, we needed access to a distribution of incoming bids. We chose to assume that we had access to every possible bid. Even though this did not give us a true distribution, making this assumption allowed us to meaningfully compare strategies. There are several ways one might run production tests to compare strategies, but this was not in the scope of our project.
    <br> <br> Our other technical challenges stem from the nature of the data we were given. The size of one week of auction data is recorded in terabytes of information, so not only did our strategies need to be feasible for very low latency use, but we also had to spend time developing efficient ways to run tests on our massive dataset without running up an astronomical AWS bill.
  takeaways: >
    My team really appreciated the freedom we were given to experiment with pricing algorithms. In particular, members with a strong interest in math or machine learning were able to develop some pretty creative techniques (although traditional approaches were generally more effective). I think most of the growth from this project stemmed from its open-endedness, and the requirement that we develop and stick to an effective timeline in the presence of such freedom.
  extra: ""
  image: none

resultcare:
  intro: >
    ResultCare is a healthcare mobile platform that allows medical professionals to browse and share information on diagnoses, tests and research papers. The goal of the project was to make the platform more social to incentivize professionals to interact with others and the app.
  problem: >
    ResultCare already had a database of thousands of research papers, diagnoses and lab tests. However, in order to make the app more than just a large mobile collection of data, the platform had to implement social aspects to keep users returning and interacting. Our team was given the main task of adding social features and integration to the existing backend and mobile application.
    / More specifically, the first half of the project involved adding social network features such as likes, followers and messaging to the ResultCare backend. The second half the project involved implementing new pages for these social features in native Android, using the endpoints that we created in the first few months.
  implementation: >
    The backend portion of the project was adding to ResultCare’s existing stack, which was mostly based in PostgreSQL and Node.js. We added endpoints to support Q&A, recent activity tracking, followers for users, reviews for lab tests and messaging between users. The native Android portion was completed using Android Studio, Java and XML. We created Android activities for messaging, user profile, community and reviews, along with infrastructure to communicate with the new backend API endpoints.
    / Another technical challenge was connecting the backend API endpoints to the Android application. Figuring out how to make API requests in parallel and properly parse the data was a much larger task than we initially anticipated, since we had to ensure both security and request speed. Native Android is definitely a challenging platform to work with and we all learned a lot from the experience.
  challenges: >
    One major technical challenge was building upon ResultCare’s existing applications and codebase. Their Android application, initially built several years ago, was still on a very old version of Android, which meant that we had to work with the older framework and features. As well, their backend functions and existing API were difficult to understand at first, which slowed down development, especially in the first few weeks.
  takeaways: >
    In general, ResultCare was an amazing learning opportunity for all the members involved, as well as a great opportunity for the company to increase campus presence and finish features for a full launch. There are definitely areas I wish I planned better and certain tasks I underestimated, but overall I believe that the team gained a lot of insight into working with startups, effective client communication and software engineering as an industry. Currently, the ResultCare app is in beta testing with hopes to launch to Android and iOS markets soon.
  extra:
  image: assets/resultcare-screenshot.png

riffyn:
  intro: Riffyn aims to make the development process easier for researchers in industry and academia by providing a cloud-based suite of products for analytics and experimentation. The objective of our project was to create an analytics web app where users could upload csv files of data and visualize the results of different kinds of statistical analysis.
  problem: Typically, to do any kind of statistical analysis on their data, Riffyn users would have to use excel or download expensive and complicated software like JUMP. Riffyn wanted us to create an easy to use platform where their users could conduct and visualize some simple analytics like linear regression and lasso regression without the need for these external tools.
  implementation: The web app was built in Dash and used Plot.ly for the visualizations. As the app’s initial use was intended to for quick visualizations that did not need to be saved, we did not need to connect to a database. We used numpy to perform the statistical analysis.
  challenges: The primary challenge in this project was figuring out the best way to visualize the statistical analyses. We needed to provide the visualizations in a way that was meaningful for the users, and not difficult to comprehend. For the regressions, we decided to show a separate visualization for each independent variable so the users could easily see which variable was significant and focus on that particular graph.
  takeaways: We had an awesome experience working with Riffyn! Everyone on the Riffyn team was an expert in their field (ranging from software to chemistry and biology) and we learned a lot from interacting with them. On the development side, the team learned a lot about different kinds of statistical analysis and also how real researchers would use this kind of product.
  extra: ""
  image: none

visumenu:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

yituux:
  intro: >
    YiTuuX develops and markets artificial intelligence and machine learning based tools to enable healthcare workers. The increased use of modern medical equipment, lab tests, etc. has created an information explosion. Physicians have to deal with huge amounts of new information, not only to stay current with the latest developments in medicine, but also to study the growing amounts of information for each one of his, or her patients. YiTuuX’s tools, developed based on the latest development in artificial intelligence and deep learning to assist physicians, and to improve their productivity in diagnosing diseases. With their tools, physicians’ productivity can be easily improved by over 50%, allowing them to focus on more difficult cases and provide more face to face interaction with patients.
  problem: >
    YiTuuX has been working on refining their healthtech tools, and were ready to begin deploying one of their core neural network models. However, AI/ML models are generally notoriously difficult to deploy correctly. YiTuuX wanted Codebase to assist in developing a prototype backend queuing system that was able to split up incoming jobs across multiple machines and a frontend from which users were able to submit jobs to the system. Another goal was to figure out how to handle jobs of incoming priorities so YiTuuX could offer users tiers of service and even corresponding SLAs in the future.
  implementation: >
    We ended up using the incredibly robust services provided by AWS (SQS, EC2, ElasticBeanstalk, S3, etc.) and Django to build out the backend. Users were able to upload jobs to a Django webserver running on Elastic Beanstalk, from which data about the jobs would be sent to SQS and S3. Jobs would be sent to particular queues based on the user’s service plan, and each EC2 instance was running a custom worker script that watched the SQS queues for new jobs, processed them, and uploaded results back to the Django webserver and S3. Throughout this process, users were able to monitor the process of their jobs using a particular URL on the Django server.
  challenges: >
    One of the interesting technical challenges we ran into was trying to get SLAs to work reliably for different tiers of users. After looking extensively at YiTuuX’s particular case, we determined that this would be nearly impossible to do cost-effectively without autoscaling our EC2 instance clusters. Since designing and developing a robust autoscaling backend unfortunately required time that we did not have for the project, we decided to go with the next best thing - designing a custom job-choosing scheme that favored jobs with higher priority.
  takeaways: >
    Overall this project was a successful learning experience - our developers were able to pick up new skills from each other and the clients were able to build a relationship with Codebase. As a team, we learned a lot about what kinds of things are important for a project to run smoothly - from team internals to client engagement and communication. Overall, the team was able to deliver a working, successful product, while also gaining a lot of insight into software development at startup companies and engineering consulting as a whole. YiTuuX will be able to build off of our ideas and prototype product when deploying their models for customers.
  extra:
  image: assets/yituux-screenshot.png

crowdbotics:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

ongo:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

sutro:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

upstream:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none

zumper:
  intro: >
    Zumper is an online platform for apartment rentals. They are focused on building a smooth, efficient, and transparent renting process for both tenants and landlords.
  problem: >
    The main method of communication between tenants and landlords on Zumper is through messaging: potential tenants can look through various apartments on the site and directly message anyone whose apartment they’re interested in. However, since Zumper as a platform is open to everyone, landlords must sometimes deal with spam messages. Our problem was to implement a spam detection solution for Zumper’s platform, improving the user experience for both tenants and landlords.
    <br><br>
    We did this through two sub-projects. For the first, we researched and implemented various ML models for spam classification. We used these models to create an API to classify any incoming messages from Zumper. No ML model is perfect, however, and we also needed to be wary of false positives (classifying an incoming message which is not spam as spam) since that would be especially bad for Zumper. Therefore, we also wanted a way to validate our model’s classifications and allow humans to step in for especially hard to classify messages. For the second part of the project, we created a web application for manual classification of messages. This allowed Zumper employees to log in and classify messages as spam or not spam, adding a human verification step to our process.
  implementation: >
    Our project came in 2 parts, web and ML. For the web app, we built out a React frontend with an Express backend connected to a PostgreSQL database. On the ML side, we trained Scipy and Tensorflow models trained on public datasets through the use of BERT, ELMo, and TF-IDF vectorization. We built a Flask application to host these models and dockerized it for deployment.
  challenges: >
    On the web side, our authentication flow added some complexity to our development process. We chose to use a Session based authentication model, and interfacing client side Session storage with backend verification while using Google OAuth to allow users to easily have access to the platform was a hurdle that we had to overcome.
    <br><br>
    One technical challenge we encountered on the ML side was ensuring we had a good experimentation pipeline with our sample data. This involved creating processes to clean and transform our message data, train our models, and finally test and visualize how our models performed compared to each other. Ensuring the pipeline was set up properly made it much easier to set up and test new models. This was also incredibly valuable to Zumper themselves, as it provided them a means to reproduce our results as well as easily experiment with their own models.
  takeaways: >
    One of our biggest takeaways from running this project was the importance of detailed planning.  Spending a lot of time before the project started to flesh out our designs and come up with week-to-week goals made the project run super smoothly overall. Having a concrete idea of our goals for each step along the way also made it easier to have a flexible development process: we were able to easily allocate more developer time when encountering technical issues and work on our stretch goals when running ahead of schedule.
  extra: ""
  image: none

zymergen:
  intro: ""
  problem: ""
  implementation: ""
  challenges: ""
  takeaways: ""
  extra: ""
  image: none